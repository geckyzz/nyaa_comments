name: Scrape AnimeTosho Comments

on:
  schedule:
    - cron: '*/10 * * * *'  # Every 10 minutes
  workflow_dispatch:  # Allow manual trigger
    inputs:
      keywords:
        description: 'Keywords to filter (comma-separated, e.g., "[ToonsHub],[EMBER],[SubsPlease]")'
        required: false
        type: string
        default: ''
      max_pages:
        description: 'Maximum pages to scrape (0 = unlimited)'
        required: false
        type: number
        default: 5
      dump_comments:
        description: 'Initialize database without sending notifications'
        required: false
        type: boolean
        default: false
      upload_db:
        description: 'Upload encrypted database to Catbox Litterbox'
        required: false
        type: boolean
        default: false
      db_expiry:
        description: 'Database upload expiry time'
        required: false
        type: choice
        options:
          - '1h'
          - '12h'
          - '24h'
          - '72h'
        default: '12h'

concurrency:
  group: animetosho-scraper
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.x'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install .
      
      - name: Restore database from cache
        uses: actions/cache/restore@v4
        with:
          path: database.at.json
          key: animetosho-database-${{ github.run_id }}
          restore-keys: |
            animetosho-database-
          
      - name: Run AnimeTosho scraper
        env:
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          DISCORD_SECRET_WEBHOOK_URL: ${{ secrets.DISCORD_SECRET_WEBHOOK_URL }}
          AT_KEYWORDS: ${{ secrets.AT_KEYWORDS }}
          AT_MAX_PAGES: ${{ secrets.AT_MAX_PAGES }}
        run: |
          CMD="python comment_scraper.py https://animetosho.org/comments"
          
          # Handle keywords from manual input or secrets
          KEYWORDS="${{ github.event.inputs.keywords }}"
          if [ -z "$KEYWORDS" ] && [ -n "$AT_KEYWORDS" ]; then
            KEYWORDS="$AT_KEYWORDS"
          fi
          
          if [ -n "$KEYWORDS" ]; then
            # Split comma-separated keywords and add -k flag for each
            IFS=',' read -ra KEYWORD_ARRAY <<< "$KEYWORDS"
            for keyword in "${KEYWORD_ARRAY[@]}"; do
              # Trim whitespace
              keyword=$(echo "$keyword" | xargs)
              if [ -n "$keyword" ]; then
                CMD="$CMD -k \"$keyword\""
              fi
            done
          fi
          
          # Handle max_pages from manual input or secrets
          MAX_PAGES="${{ github.event.inputs.max_pages }}"
          if [ -z "$MAX_PAGES" ] && [ -n "$AT_MAX_PAGES" ]; then
            MAX_PAGES="$AT_MAX_PAGES"
          fi
          if [ -n "$MAX_PAGES" ]; then
            CMD="$CMD --max-pages $MAX_PAGES"
          fi
          
          # Handle dump_comments flag
          if [ "${{ github.event.inputs.dump_comments }}" = "true" ]; then
            CMD="$CMD --dump-comments"
          fi
          
          # Handle upload_db flag
          if [ "${{ github.event.inputs.upload_db }}" = "true" ]; then
            CMD="$CMD --upload-db --db-expiry ${{ github.event.inputs.db_expiry || '12h' }}"
          fi
          
          echo "Running: $CMD"
          eval $CMD
      
      - name: Save database to cache
        uses: actions/cache/save@v4
        if: always()
        with:
          path: database.at.json
          key: animetosho-database-${{ github.run_id }}
